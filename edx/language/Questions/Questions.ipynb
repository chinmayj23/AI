{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Questions.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMKImWKYSB41G/96miyJaIg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chinmayj23/AI/blob/master/edx/language/Questions/Questions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NeNJfYefBqW"
      },
      "source": [
        "import nltk\n",
        "import sys\n",
        "import os\n",
        "import string\n",
        "import math"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "316-SnC3fGWr"
      },
      "source": [
        "\n",
        "FILE_MATCHES = 4\n",
        "SENTENCE_MATCHES = 1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKJhzPpTfJEB"
      },
      "source": [
        "def main():\n",
        "\n",
        "    # Check command-line arguments\n",
        "    if len(sys.argv) != 2:\n",
        "        sys.exit(\"Usage: python questions.py corpus\")\n",
        "\n",
        "    # Calculate IDF values across files\n",
        "    files = load_files(sys.argv[1])\n",
        "    file_words = {\n",
        "        filename: tokenize(files[filename])\n",
        "        for filename in files\n",
        "    }\n",
        "    file_idfs = compute_idfs(file_words)\n",
        "\n",
        "    # Prompt user for query\n",
        "    query = set(tokenize(input(\"Query: \")))\n",
        "\n",
        "    # Determine top file matches according to TF-IDF\n",
        "    filenames = top_files(query, file_words, file_idfs, n=FILE_MATCHES)\n",
        "\n",
        "    # Extract sentences from top files\n",
        "    sentences = dict()\n",
        "    for filename in filenames:\n",
        "        for passage in files[filename].split(\"\\n\"):\n",
        "            for sentence in nltk.sent_tokenize(passage):\n",
        "                tokens = tokenize(sentence)\n",
        "                if tokens:\n",
        "                    sentences[sentence] = tokens\n",
        "\n",
        "    # Compute IDF values across sentences\n",
        "    idfs = compute_idfs(sentences)\n",
        "\n",
        "    # Determine top sentence matches\n",
        "    matches = top_sentences(query, sentences, idfs, n=SENTENCE_MATCHES)\n",
        "    for match in matches:\n",
        "        print(match)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FR84nX6fMv_"
      },
      "source": [
        "def load_files(directory):\n",
        "    \"\"\"\n",
        "    Given a directory name, return a dictionary mapping the filename of each\n",
        "    `.txt` file inside that directory to the file's contents as a string.\n",
        "    \"\"\"\n",
        "    corpus = dict()\n",
        "\n",
        "    for filename in os.listdir(directory):\n",
        "        file_path = os.path.join(directory, filename)\n",
        "        if os.path.isfile(file_path) and filename.endswith(\".txt\"):\n",
        "            with open(file_path, \"r\", encoding='utf8') as file:\n",
        "                corpus[filename] = file.read()\n",
        "\n",
        "    return corpus"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhLrNyWifPnX"
      },
      "source": [
        "def tokenize(document):\n",
        "    \"\"\"\n",
        "    Given a document (represented as a string), return a list of all of the\n",
        "    words in that document, in order.\n",
        "    Process document by coverting all words to lowercase, and removing any\n",
        "    punctuation or English stopwords.\n",
        "    \"\"\"\n",
        "    words = nltk.word_tokenize(document)\n",
        "\n",
        "    return [\n",
        "        word.lower() for word in words\n",
        "        # Filter out any stopword\n",
        "        if word not in nltk.corpus.stopwords.words(\"english\")\n",
        "        # Filter out any word that only contains punctuation symbols ('-', '--') but not ('self-driving')\n",
        "        and not all(char in string.punctuation for char in word)\n",
        "    ]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQavPR0vfTI6"
      },
      "source": [
        "def compute_idfs(documents):\n",
        "    \"\"\"\n",
        "    Given a dictionary of `documents` that maps names of documents to a list\n",
        "    of words, return a dictionary that maps words to their IDF values.\n",
        "    Any word that appears in at least one of the documents should be in the\n",
        "    resulting dictionary.\n",
        "    \"\"\"\n",
        "    counts = dict()\n",
        "\n",
        "    for filename in documents:\n",
        "        seen_words = set()\n",
        "\n",
        "        for word in documents[filename]:\n",
        "            if word not in seen_words:\n",
        "                seen_words.add(word)\n",
        "                try:\n",
        "                    counts[word] += 1\n",
        "                except KeyError:\n",
        "                    counts[word] = 1\n",
        "\n",
        "    return {word: math.log(len(documents) / counts[word]) for word in counts}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2ajTTEvfXOK"
      },
      "source": [
        "def top_files(query, files, idfs, n):\n",
        "    \"\"\"\n",
        "    Given a `query` (a set of words), `files` (a dictionary mapping names of\n",
        "    files to a list of their words), and `idfs` (a dictionary mapping words\n",
        "    to their IDF values), return a list of the filenames of the the `n` top\n",
        "    files that match the query, ranked according to tf-idf.\n",
        "    \"\"\"\n",
        "    tf_idfs = dict()\n",
        "\n",
        "    for filename in files:\n",
        "        tf_idfs[filename] = 0\n",
        "        for word in query:\n",
        "            tf_idfs[filename] += files[filename].count(word) * idfs[word]\n",
        "\n",
        "    return [key for key, value in sorted(tf_idfs.items(), key=lambda item: item[1], reverse=True)][:n]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4z3IUmgzfaSK"
      },
      "source": [
        "def top_sentences(query, sentences, idfs, n):\n",
        "    \"\"\"\n",
        "    Given a `query` (a set of words), `sentences` (a dictionary mapping\n",
        "    sentences to a list of their words), and `idfs` (a dictionary mapping words\n",
        "    to their IDF values), return a list of the `n` top sentences that match\n",
        "    the query, ranked according to idf. If there are ties, preference should\n",
        "    be given to sentences that have a higher query term density.\n",
        "    \"\"\"\n",
        "    rank = list()\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence_values = [sentence, 0, 0]\n",
        "\n",
        "        for word in query:\n",
        "            if word in sentences[sentence]:\n",
        "                # Compute “matching word measure”\n",
        "                sentence_values[1] += idfs[word]\n",
        "                # Compute \"query term density\"\n",
        "                sentence_values[2] += sentences[sentence].count(word) / len(sentences[sentence])\n",
        "\n",
        "        rank.append(sentence_values)\n",
        "        \n",
        "    return [sentence for sentence, mwm, qtd in sorted(rank, key=lambda item: (item[1], item[2]), reverse=True)][:n]"
      ],
      "execution_count": 8,
      "outputs": []
    }
  ]
}